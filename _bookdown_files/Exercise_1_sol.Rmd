---
title: "AEF_Exercise1"
author: "Nikolas Anic, Lorenz Gassmann"
date: "11/9/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The Markdown files are used to test your knowledge on the subjects taught throughout week 2 and 3 of the Exercise and Lab sessions as well as on the theoretical topics you received during the lecture. Please fill the file such that you enter your answers in the respective code chunks below each question. If you require more code chunks to complete your assignment, please put the chunks in the correct position and indicate each chunk accordingly. 

**Deadline**

All deliverables have to be turned in on OLAT by the **04th of March, 2022, 12am (noon) before class** the latest. Ensure that you submit the deliverables before the deadline, since late reports will receive a grade of 1.

**Deliverables**

- **R Markdown File**: Turn in the R-Markdown file including all your calculations and relevant comments for the code and the answers to the theoretical questions. The file should be named as followed: `groupname assignment2.Rmd`

**Grading**

The grading of the assignment will be based on three main evaluation parts. 

1. Theoretical Knowledge and Accuracy of its implications 

In the first part, we will test your knowledge on the theoretical topics covered during the lecture and assess your ability to transfer the theoretical arguments to concrete use cases. This evaluation will be in form of intuition-and theory-based argumentation questions, which will either rely on statements derived from class or from quantitative outputs of related papers. You do not explicitly need to read the papers in order to comprehend the question. However, note that it is recommended to at least skim through it as it will also enhance your ability to solve the questions. Please answer the questions in the marked fields and use, whenever necessary, Latex language to convey your statements (e.g. use $\beta_i$ to indicate the i'th $\beta$ or use `$$...$$` to write an equation)

2. Application of the theory and Lab exercises

In the second part, we will test your ability to apply the theoretical concepts taught during the lecture in actual code and evaluate the extent to which you are able to transfer theory questions into real-life use cases. This assessment will be in form of coding-and-describing questions, which will rely to approximately 70% on the coding steps and concepts you can find in the **Bookdown** file, and to 30% on your own ability to enhance the provided code and advance the empirical steps we discussed during the Lab sessions. Note that **you can copy paste code we provide you with** to solve the exercises, but be aware that we may have **changed the inputs and procedures for some questions**, such that **simply copy pasting the code may sometimes result in an incorrect answer**. 

3. Design and Documentation 

The last part constitutes an evaluation of your overall ability to design and document code. Therein, we will assess both the theoretical and empirical part of your submission according to the design choices and extent of the documentation. That is, we will evaluate based on the (I) documentation of your coding steps, (II) structure of your code, (III) structure of the entire document, (IV) description of the theory questions as well as (V) design choices of your plots. We follow this approach because effective coding is not only correct, but also comprehensible and well documented. Consequently, writing with consistent and coherent designs of your code is a necessary prerequisite when working with multiple stakeholder groups and we want you to be able to mititgate potential irregularities that arise from individually written code. 

**Code and Copy**

As we already mentioned, you are allowed to copy paste code we provided you with as long as you ensure that this code is actually correct and adjusted to the question at hand. However, note that we want you to code on your own. This means that we neither want you to copy code you received from your peers that took the class in previous years nor copy it from peers that take it this year. In order to ensure a fair and transparent evaluation, we thus will run all your codes through a code copy identifier, which provides us with similarity scores for each code. If some similarity should be above a certain threshold, we will check the repsective assignments and, if we deem that there is a copy issue, we may address you directly. This is likely to be in form of a small oral assignment in which you are required to tell us all the code steps and how they relate to each other. Note that we don't want to punish you for cheating, but rather we want to to understand the code behind your solution. Consequently, even if you copied your code and we find out, you will have to look at the code again and try to comprehend it, fostering your knowledge either way :-)

**Materials**

We will provide you with a template for the exercise session. You can either choose to work with this template or to use your own created template. Further, we provide you with a comprehensive list of R packages. These packages will allow you to work on most of the tasks you will ever encounter in R and thus serve as a sound fundament for your future work. Note that although we do not require all the packages we provide you with, it nevertheless is handy to call all of the packages first such that you don't have to run into errands later on. 

```{r, comment=NA, include=FALSE, echo=FALSE}
packs.inst <- c("readxl","foreign","dplyr","tidyr","ggplot2","stargazer","haven","dummies","Hmisc",
           "lmtest","sandwich", "doBy", "multiwayvcov", "miceadds", 
           "car", "purrr", "knitr", "zoo", "readstata13", "tidyverse", "psych",
           "wesanderson", "lubridate","reporttools", "data.table", "devtools",
           "rmarkdown","estimatr", "ivpack", "Jmisc", "lfe", "plm", "tinytex", "xts", "psych", "PerformanceAnalytics",
           "roll", "rollRegres", "glmnet", "hdm", "broom", "RCurl", "learnr", "maps", "fGarch", "remotes", "RPostgreSQL", "wrds", "DBI", "RPostgreSQL", "remotes", "RPostgres", "Rmisc", "ggthemes", "splitstackshape", "gginference", "MASS")
packs.load <- c("fGarch")


# lapply(packs.inst, install.packages, character.only = FALSE) 

lapply(packs.inst, require, character.only = TRUE)
```

Remember that, if you have never installed any package before, you need to use the commented out function first. Further, if you encounter any issues while installing the packages, look which package rendered an error and exclude this package. Also note that, during the first installing process, it may take a while to get all the packages. 

# Data

On OLAT you will find *five different data sets* for the first assignment. You will need to use these datasets in order to tackle Exercise 9. The datasets are labeled as 

- **BigFour Dataset**

This data set contains the historical prices of the big four companies (Nestle, Novartis, Roche and UBS). You should use this dataset to calculate time-series returns in an xts object. 


- **File1 - File5**

- These are five randomly generated datasets with 33 x and y observations. You should use this dataset for Exercise 9. 

# Controls and Hints

Below you find sample solutions for certain values to check your work progress and make sure that you are on the right track:


# Exercise 1: R Markdown files - Handling of code chunks and white spaces in Latex style 

In the first exercise, you will work with the R markdown file. Therein, we ask you to perform preliminary steps in order to better understand how to work with both code chunks and the white spaces. Further, you will be asked to work on your Latex style grammar. 

## Adjust the text below such that: 

- The first sentence is written in italic
- The second sentence is written in in bold
- The third sentence is written underlined
- The first word in the fourth sentence is given as verbatim
- The words defined as (I), (II) etc. of the fifth sentence are given in an enumeration 

Text: 

White spaces are the spaces you are currently reading this text. They serve as string-based Chunks, meaning that you can write human language based sentences within these cells. They are one of the two main methods to write something into R Markdown. Codechunks are the other form of writing into word. White spaces serve as areas for writing (I) results of reports, (III) explanations, (IV) formulas or  (V) in general everything that has no code-based output at the end.

## Adjust the code chunk settings

For the following code chunk:

- change both the figure width to 3 and the figure height to 2.3
- change the alignment to center

Further, change the code chunk such that:

- R hides the code in the output
- prevents messages and warnings from appearing in the finished file

```{r, fig.height = 2.3, fig.width = 3, fig.align='center', echo=FALSE, warning=FALSE}
plot(datasets::swiss$Fertility, datasets::swiss$agriculture)
```

## Write the formula for the Normal Distribution with Dollar Signs around it (= $$...$$)

[The Normal Distribution is defined as ](https://www.thoughtco.com/normal-distribution-bell-curve-formula-3126278)

$$
y = frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

## Write the proof of a function with the "align" method

Go to the [following website](http://sp-finance.e-monsite.com/pages/important-concept/mathematical-concepts/ito-s-lemma.html). Search for the sentence "is of order dt, it does not integrate to zero and must remain in the equation." Below this sentence you will find one part of the proofs for Ito's Lemma, which is used for the Brownian Motion (it is a proof starting with df = ... and has 5 rows overall). 

You are now asked to write the proof in latex format. Remember that you need to use the $$...$$ signs and the "align" method: 

$$
\begin{align}
df &= \frac{\delta f}{\delta X_t}dX_t + \frac{\delta f}{\delta t}dt + \frac{1}{2} \frac{\delta^2 f}{\delta X_t^2}(dX_t)^2 \\
&= \frac{\delta f}{\delta X_t} (a_tdt + b_tdW_t) + \frac{\delta f}{\delta t}dt + \frac{1}{2} \frac{\delta^2 f}{\delta X_t^2}(a_tdt + b_tdW_t)^2 \\
&= \left(\frac{\delta f}{\delta X_t}a_t + \frac{\delta f}{\delta t}dt + \frac{1}{2} \frac{\delta^2 f}{\delta X_t^2}b_t^2 \right)dt + \frac{\delta f}{\delta X_t}b_tdW_t
\end{align}
$$

# Exercise 2: Vectors 

## Create a vector of three elements (1,5,8) and name that vector `vec_a`. Create a second vector, `vec_b`, that contains (6,7,11). Add these two vectors together and name the result `vec_c`. 

```{r}
vec_a <- c(1,5,8)
vec_b <- c(6,7,11)
vec_c <- vec_a + vec_b
```

## Create a vector, named `vec_d`, that contains only two elements (14,18). Add this vector to `vec_a`. What is the result and what do you think R did? What is the warning message that R gives you?

```{r}
vec_d <- c(14,18)
vec_a + vec_d
```

## Generate the vector of integers: {3,6,...,30}  in two different ways:

- First using the `seq()` function
- Using the `a:b` shortcut and subsequent algebra

```{r}
seq_vec <- seq(3,30, 3)
```

## Generate the vector  {2,4,8,2,4,8,2,4,8} using the rep() command to replicate the vector c(2,4,8)

```{r}
rep_vec <- rep(c(2,4,8), 3)
```

## The vector letters is a built-in vector to R and contains the lower case English alphabet.

- Extract the 11th element of the letters vector.
- Extract the sub-vector that contains the 9th, 12th, and 22nd elements.
- Extract the sub-vector that contains everything except the last two elements and the fifth element 

```{r}
l_11 <- letters[11]
l_sub <- letters[c(9,12,22)]
l_sub2 <- letters[-c(5,25,26)]
```

# Exercise 2: Matrices and Data Frames

## Matrices

Work with the following matrix: 

$$
M= 
\begin{bmatrix}
1 & 8 & 7 & 18 & 10\\
3 & 5 & 44 & 7& 16 \\
5& 3 & 12 & 3 & 6  
\end{bmatrix}
$$

### Create the matrix in two ways and save the resulting matrix as M:

- Create the same matrix by either the rbind() or cbind() command.

```{r}
mat_a <- cbind(c(1,3,5), c(8,5,3), c(7,44,12), c(18,7,3), c(10,16,6))
```


### Use the matrix for manipulation: 

- Extract the second element of the third column

```{r}
mat_a[2,3]
```

- Extract the last row 

```{r}
mat_a[3, ]
```

## Data Frames

## Create the following data frame:

  - Company: ABB, Nestle, Roche, Novartis, UBS, Credit Suisse, Actelion
  - ROE: 0.12, 0.21, 0.04, 0.32, 0.10, 0.23, 0.24
  - Rating: AAA, AA, AAA, AA+, BB, BBB, CC
  - Size: 200, 1800, 1124, 1341, 1211, 900, 123

### Extract the third observation (i.e. the third row)

```{r}
df_1 <- data.frame(Company = c("ABB", "Nestle", 'Roche', 'Novartis', 'UBS', 'Credit Suisse', 'Actelion'), 
              ROE = c(0.12, 0.21, 0.04, 0.32, 0.10, 0.23, 0.24), 
              Rating = c('AAA', 'AA', 'AAA', 'AA+', 'BB', 'BBB', 'CC'), 
              Size = c(200, 1800, 1124, 1341, 1211, 900, 123))

df_1[3,]
```

### Create an order that orders the rating column according to: AAA - AA+ - AA - BB - BBB - CC

```{r}
df_1 <- data.frame(Company = c("ABB", "Nestle", 'Roche', 'Novartis', 'UBS', 'Credit Suisse', 'Actelion'), 
              ROE = c(0.12, 0.21, 0.04, 0.32, 0.10, 0.23, 0.24), 
              Rating = factor(c('AAA', 'AA', 'AAA', 'AA+', 'BB', 'BBB', 'CC'), levels = c("AAA", "AA+", "AA", "BBB", "BB", "CC", "D")),
              Size = c(200, 1800, 1124, 1341, 1211, 900, 123))
```

### Print out a data frame of all the observations except for the fourth observation. (i.e. Remove the fourth observation/row.)

```{r}
df_1[-4,]
```

###  Use the `which()` command to create a vector of row indices that have a size greater than or equal to 1000 Call that vector Sufficient_Rating (= dummy variable)

```{r}
indices_vec <- c(which(df_1$Size >= 1000))
```

### Create a dummy variable with the `ifelse()` command that indicates 1 if the size is above the mean and 0 else

```{r}
df_1$dummy_size <- ifelse(df_1$Size > median(df_1$Size), 1, 0)
```

### Get the average size of the companies for which both dummies are 1 (above mean size and rating greater than or equal to AA)

```{r}
mean(subset(df_1, dummy_size == 1)$Size)
```

# Exercise 3: Importing Data

1. Get the Swiss Fertility and Socioeconomic Indicators (1888) Dataset. Google it and try to get the correct R package which includes the dataset, by using: `specific_package_name::swiss`. 

2. Calculate the average fertility rate for all communes under consideration if the education level is above 7. 

```{r}
swiss <- datasets::swiss
mean(swiss$Fertility)
```

# Exercise 4: Data Manipulation

In this part, we want you to work in the `dplyr` way that we showed you during the first exercise session. Please refer to the section in the bookdown if you have any questions as how to write the code in the appropriate fashion.  

## Get and load the dataset on `UScrime` from the `MASS()` package and assign a name to it

```{r}
UScrime <- MASS::UScrime
```

## Select all except the columns Po1 and M.F. Assign a name to this shrunken data frame

```{r}
UScrime_shrunken <- UScrime %>% select(-c(Po1, M.F))
```

## Print all rows for which Pop is below 50 and NW is above 100

```{r}
UScrime_shrunken %>% filter(Pop < 50 & NW > 100)
```

## Order the dataframe according to the first column - but in descending format

```{r}
UScrime_shrunken %>% arrange(desc(M))
```

## Create a new variable that multiplies the column So with Ed and create in the same code a new column that assigns a 2 if Ed is above 90 and a 0 otherwise

```{r}
UScrime_shrunken <- UScrime_shrunken %>% mutate(SoEd = So*Ed) %>% mutate(Ed_Large = ifelse(SoEd > 90, 2, 0))
```

## Now, perform some further manipulation strategies simultaneously

- 1) Select all except for the "So", "Prob" and "y" column
- 2) Filter according to population below 50, GDP above 400 and Ineq below the median value
- 3) Create another dummy variable that assigns a 1 if LF is above the mean value and a 0 otherwise
- 3) Group by this dummy variable
- 4) Create variables for the max, min and mean values of the column "Ed" for each group

```{r}
UScrime_shrunken %>% 
  select(-c(So, Prob, y)) %>% 
  filter(Pop < 50 & GDP > 400 & Ineq < median(Ineq)) %>% 
  mutate(dummy_LF = ifelse(LF > mean(LF), 1, 0)) %>% 
  group_by(dummy_LF) %>% 
  mutate(min_Ed = min(Ed), max_Ed = max(Ed), mean_Ed <- mean(Ed))
```

# Exercise 5: GGPlot

##  Get and load the dataset on `UScrime` from the `data()` package. 

## Create a histogram of the column Ineq

```{r}
UScrime %>% ggplot(aes(x = Ineq)) + geom_histogram()
```

## Plot a scatterplot of NW and Ineq 

```{r}
UScrime %>% ggplot(aes(x = NW, y = Ineq)) + geom_point()
```

## Distinguish the scatterplot between So = 1 and So = 0s with the `color` argument

```{r}
UScrime %>% ggplot(aes(x = NW, y = Ineq, color = So)) + geom_point()
```

## Add a linear line to the plot 

```{r}
UScrime %>% ggplot(aes(x = NW, y = Ineq)) + geom_point() + geom_smooth(method = "lm")
```

## Calculate the mean and 95% CI values for Ineq depending on So. Plot an errorbar as we did in the exercise session with the mean value for Ineq as well as a ribbon indicating the 95% CI for So = 0 and So = 1

```{r}
UScrime <- UScrime %>% 
  # Group by classes 
  group_by(So) %>% 
  # Get the mean and sd values for the cty variable based on its class
  mutate(mean_ineq = mean(Ineq)) %>% 
  mutate(sd_ineq = sd(Ineq)) %>% 
  # Create lower and upper bounds per class 
  mutate(lwr_ineq = mean_ineq - 1.96*sd_ineq) %>% 
  mutate(upr_ineq = mean_ineq + 1.96*sd_ineq)

# Define the error bar plot for each class
UScrime %>% ggplot(aes(x = So)) + # Still get in the x axis the individual classes
  # Get the error bars per class
  geom_errorbar(aes(ymin=lwr_ineq, ymax=upr_ineq)) + 
  # Get the mean points per class
  geom_point(aes(y = mean_ineq), col = "blue", size=3, shape=21, fill="white")
```

## Melt the Po1 and Po2 columns. Plot a density plot and histogram for the variables as we did in the lecture

```{r, warning = FALSE}
Po12<- UScrime[,c("Po1", "Po2")]
melt_Po12 <- melt(Po12)

# This creates of the two columns wide format a one column long format which summarises both columns (indicated as value column). Based on this, we have a column that indicates which of the two former columns is assigned to which value, to now be able to plot multiple column density plots.  
melt_Po12 %>% ggplot(aes(x = value, fill = variable)) +
  geom_density(alpha = 0.2)
```

## Make a facet grid plot of NW and Ineq. Do this based on 1) So and 2) on a variable you create that splits GDP into 4 groups. For this, use the `cut()` function with the following command: `mutate( GDP_Groups = cut(GDP, breaks = c(318, 420, 550, 689))`

```{r}

UScrime <- UScrime %>% 
  mutate(GDP_Groups = cut (GDP, breaks = c(318, 420, 550, 689))) 

UScrime %>% ggplot(aes(x = NW, y = Ineq)) + geom_point() + facet_grid(So ~ GDP_Groups)
```

## Now make the plot that you created from 2. - 5. pretty. That is, orientate yourself on the design principles and code we provided you with and make a nice looking plot

```{r}
# Please add your answer here
```

# Exercise 6: Date and Times

## For the following formats for a date, transform them into a date/time object. Which formats can be handled nicely and which are not?

```{r}
birthday <- c(
  'May 31st, 1996',
  'May 31, 1996',
  'Sep 13, 1978',
  '5-31-96',
  '5/31/96', 
  '31. Mai 1996')
```

```{r}
mdy(birthday)
```

## Time and Date: For the following data, create the correct US date and time format:

```{r}
date_time <- c("31st May 1996 16:05:11", "31.05.1996 16:05:11", "31 May 96 16:05:11", "31 May 1996 16:05:11", "31-5-96 16:05:11", "31-5-1996 16:05:11")
```

```{r}
as.Date(dmy_hms(date_time))
```

## For the data above, just take the date, but leave the time out

# Exercise 7: Time Series Data

Load the data on the big four companies that we have provided you with. For this dataset: 

## Transform the dataset into an xts object 

```{r}
# Please add your answer here
```

## Calculate a lag value for each time-series column. Write down quickly why we should use the command `stats::lag()` when we want to define a lagged value. 

```{r}
# Please add your answer here
```

# Exercise 8: 

## The following file names were used in a camera trap study. The S number represents the site, P is the plot within a site, C is the camera number within the plot, the first string of numbers is the YearMonthDay and the second string of numbers is the HourMinuteSecond.

```{r}
file.names <- c( 'S123.P2.C10_20120621_213422.jpg',
                 'S10.P1.C1_20120622_050148.jpg',
                 'S187.P2.C2_20120702_023501.jpg')
```

Use a combination of `str_sub()` and `str_split()` to produce a data frame with columns corresponding to the *site, plot, camera, year, month, days, hour, minute, and second* for these three file names. So we want to produce code that will create the data frame:

$$
\begin{matrix}
Site & Plot & Camera & Year & Month & Day & Hour & Minute & Second \\
S123  & P2   & C10 &2012  &  06  &21   &21  &   34   &  22\\
S10  & P1   &  C1 &2012   & 06 & 22  & 05  &   01   &  48\\
S187&   P2   &  C2 &2012  &  07&  02 &  02  &   35  &   01
\end{matrix}
$$

```{r}
# 
```

# Exercise 9: Loading multiple datasets with paste and assign

## Now, we want to load different datasets and assign them distinct names. For that, use the 5 distinct csv files we provide you with, called: `file1` to `file5`. Load each of the files and assign them the names `Data1` to `Data5` in the R environment. 

```{r}
data_names <- c("A1_dataset_01", "A1_dataset_02", "A1_dataset_03", "A1_dataset_04", "A1_dataset_05")

# Now, we have the names. We now need to tell R to:

## loop through each element of these names
## paste each element together with the path structure to get the file path of each csv file 
## run the csv_read() command for the respective filepath to read in each file 
## assign a name to each uploaded csv file (otherwise we would just overwrite each file over and again while running the loop, resulting in only the last file being read in and saved)

# This is one of the ways to proceed 
for (i in 1:length(data_names)){
  # First, we tell R to loop through each element in the vector "data_names" which contains the names of the csv files
  paste0(assign(paste0("Data", i),
                # Here, we create a name for the data frame we read in (in our case data1 - data7). These are the names of the read-in csv files
         # Here, we assign the created names to the individual read-in csv files (this is identical to e.g.: data1 = read.csv(...)) which is how we            would assign a name if we would read in the data manually 
         read.csv(paste0("~/Desktop/Empirical Asset Management/R/Excel/", data_names[i], ".csv"), sep = ",")))
         # Here, we read-in the data. We use another paste0 command to paste together the:
          ## 1) path to the files 
          ## 2) respective data name 
          ## 3) csv indicator
        # E.g. for the first element of the files, paste0 creates the following string:     
          ##"~/Desktop/EMMA/Excel_Files/Cumulative_Returns/Cum_Ret_MC_Gen.csv"
        # This looks identical to when we usually type in the data path and name manually. 
  
  # We repeat this for all the individual csv files at the given location. As such, we read-in seven csv files and named them data1 - data7
  
}

```

## Now, plot a scatterplot of the x and y variable for each of the distinct dataframes. Also follow the for loop logic we discussed in the Bookdown. 

```{r}
data_names_r <- list(Data1, Data2, Data3, Data4, Data5)

# Running the loop to create multiple plots is actually quite straight forward, the only thing we need to adjust the name of each plot
for (i in 1:length(data_names_r)){
  # Here, R loops through each xts object again
  assign(paste("figure_",data_names[i],sep = ""),
         # Here, we define the name again for each created ggplot
        data_names_r[[i]] %>% 
  # Here, we then assign the name to the ggplot that we created, based on each tidy element of the list_df_ts (our xts objects)
    
  # Below, we just run our usual code to define some good-looking plots!
    
          ggplot(aes(x=x,y=y)) + 
          geom_point() + 
          # Line, Point and Fill colours
          scale_color_manual(values=c("goldenrod", "darkorchid4", "darkorange2","dodgerblue1", "springgreen2",  "darkorchid4", "dodgerblue4"))  +
          scale_fill_manual(values=c("goldenrod", "darkorchid4", "darkorange2","dodgerblue1", "springgreen2",  "darkorchid4", "dodgerblue4")) +  
          # X and Y axis string
          ylab("y") + xlab("x") + 
          # Title string
          ggtitle(paste("Relationship of x and y for datasst number ", i)) +
          labs(color='Factor Portfolios') +
          theme(
          # Title Elements
          plot.title= element_text(size=14, color="grey26", hjust=0.3,lineheight=0.4, margin=margin(15,0,15,0)), 
          # Axis Elements
          axis.title.y = element_text(color="grey26", size=12, margin=margin(0,10,0,10)),
          axis.title.x = element_text(color="grey26", size=12, margin=margin(10,0,10,0)),
          # Background colour and fill
          panel.background = element_rect(fill="#f7f7f7"),
          plot.background = element_rect(fill="#f7f7f7", color = "#f7f7f7"),
          # Major Panel Grids
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_line(size = 0.5, linetype = "solid", color = "grey"),
          # Minor Panel Grids
          panel.grid.minor.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          # Line colour of the x and y axis
          axis.line = element_line(color = "grey")))
}

figure_File1
```


------------------------------------------------------------------------------------------------------------------------------------------------

# Administration of Assignment 1 and Timeline 

In order to successfully complete the Assignment 1, students should have covered the topics of **Chapter 1: The programming environment**. This chapter includes subjects on R Markdown use and the most fundamental use cases in the R programming environment. 

The Assignment 1 will start **after the first Exercise Session (Week 1)**. In order to convey the topics to the students, we will cover the material in **first session held in Week 1**. 

Especially, we need to cover: 

- Working with Markdown 
- Writing Functions and Mathematical Proofs in a Latex environment
- Using Code Chunks and White Spaces
- Loading in Data from multiple sources
- Vector and Matrix manipulation
- Data Manipulation with `dplyr`
- Data visualization using `ggplot`
- Date Manipulation
- String Manipulation
- Working with string manipulation in loops
- Database Queries

We plan to take the following time amount for the first session

- Week 1: 

  - Chapter 2.1: 15 Minutes
  - Chapter 2.2: 75 Minutes
  
    - Chapter 2.2.2: 5 Minutes
    - Chapter 2.2.4: 10 Minutes
    - Chapter 2.2.5: 5 Minutes
    - Chapter 2.2.6: 20 Minutes
    - Chapter 2.2.8: 15 Minutes
    - Chapter 2.2.9: 5 Minutes 
    - Chapter 2.2.10: 15 Minutes (only the paste and assign part)


------------------------------------------------------------------------------------------------------------------------------------------------



